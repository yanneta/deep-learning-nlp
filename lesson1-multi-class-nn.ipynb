{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward NN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz\n",
    "\n",
    "wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! mkdir -p data\n",
    "    ! wget -O names_test.csv.gz https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz?raw=true\n",
    "    ! wget -O names_train.csv.gz https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz?raw=true\n",
    "    ! gunzip *.gz\n",
    "    ! mv names_test.csv names_train.csv data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-26 10:02:19--  https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz?raw=true\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/names_test.csv.gz [following]\n",
      "--2019-09-26 10:02:19--  https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/names_test.csv.gz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz [following]\n",
      "--2019-09-26 10:02:19--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.188.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.188.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27541 (27K) [application/octet-stream]\n",
      "Saving to: ‘names_test.csv.gz’\n",
      "\n",
      "names_test.csv.gz   100%[===================>]  26.90K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2019-09-26 10:02:20 (1.96 MB/s) - ‘names_test.csv.gz’ saved [27541/27541]\n",
      "\n",
      "--2019-09-26 10:02:20--  https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz?raw=true\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/names_train.csv.gz [following]\n",
      "--2019-09-26 10:02:20--  https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/names_train.csv.gz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz [following]\n",
      "--2019-09-26 10:02:20--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.188.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.188.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50237 (49K) [application/octet-stream]\n",
      "Saving to: ‘names_train.csv.gz’\n",
      "\n",
      "names_train.csv.gz  100%[===================>]  49.06K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2019-09-26 10:02:21 (1.58 MB/s) - ‘names_train.csv.gz’ saved [50237/50237]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/Genia4ERtest.tar.gz'),\n",
       " PosixPath('data/sampletest2.iob2'),\n",
       " PosixPath('data/LICENSE'),\n",
       " PosixPath('data/Genia4ERtask2.iob2'),\n",
       " PosixPath('data/names_train.csv'),\n",
       " PosixPath('data/names_test.csv'),\n",
       " PosixPath('data/Genia4EReval1.iob2'),\n",
       " PosixPath('data/Genia4EReval2.iob2'),\n",
       " PosixPath('data/sampletest1.raw'),\n",
       " PosixPath('data/sampletest2.raw'),\n",
       " PosixPath('data/README.txt'),\n",
       " PosixPath('data/Genia4ERtask1.iob2'),\n",
       " PosixPath('data/Genia4EReval1.raw'),\n",
       " PosixPath('data/Genia4EReval2.raw'),\n",
       " PosixPath('data/sampletest1.iob2'),\n",
       " PosixPath('data/Genia4ERtraining.tar.gz')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adsit\",\"Czech\"\r",
      "\r\n",
      "\"Ajdrna\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitsch\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitz\",\"Czech\"\r",
      "\r\n",
      "\"Ballalatak\",\"Czech\"\r",
      "\r\n",
      "\"Ballaltick\",\"Czech\"\r",
      "\r\n",
      "\"Bastl\",\"Czech\"\r",
      "\r\n",
      "\"Baroch\",\"Czech\"\r",
      "\r\n",
      "\"Betlach\",\"Czech\"\r",
      "\r\n",
      "\"Biganska\",\"Czech\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head data/names_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab2id and label2id\n",
    "Computing vocab2id dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "vocab2id[\" \"] # I am going to use 0 to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(x, seq_len=15, vocab2id=vocab2id):\n",
    "    x = list(x)\n",
    "    x = np.array([vocab2id[k] for k in x])\n",
    "    z = np.zeros(seq_len, dtype=np.int32)\n",
    "    n = min(seq_len, x.shape[0])\n",
    "    z[0:n] = x[0:n]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 29, 30, 30, 30,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_seq(\"aabbb\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def seq2matrix(x, vocab_len=55):\n",
    "    z = np.zeros((x.shape[0], vocab_len))\n",
    "    z[np.arange(len(x)), x] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NameDataset(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "valid_ds = NameDataset(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3, 32, 47, 37, 48,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       dtype=int32), 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_ds[0]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, vocab_size=55, emb_size=50, n_class=18, seq_len=15, hidden=50):\n",
    "        super(NN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear1 = nn.Linear( seq_len*emb_size, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, n_class)\n",
    "        self.bn = nn.BatchNorm1d(hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28, 36, 40, 49, 39, 48, 43, 50,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 3, 50, 31, 36, 37, 42, 42, 37, 39, 43, 50,  0,  0,  0,  0],\n",
       "        [ 4, 29, 30, 29, 32, 54, 36, 29, 42, 53, 29, 42,  0,  0,  0],\n",
       "        [ 9, 40, 29, 32, 31, 36, 33, 42, 39, 43,  0,  0,  0,  0,  0],\n",
       "        [16, 29, 39, 36, 43, 32, 39, 37, 42,  0,  0,  0,  0,  0,  0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 14, 14, 14, 14])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=55\n",
    "emb_size=50\n",
    "emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 50])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x.long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 750])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x.long()).view(x.size(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6068, -0.1797,  0.1763, -1.1019,  0.6065,  0.2611, -0.0560, -0.0980,\n",
       "          0.1155, -0.2292,  0.9285,  0.2749, -0.6643,  0.2179,  0.0859,  0.9098,\n",
       "          0.5109,  0.4601],\n",
       "        [-0.5979, -0.7824,  0.3607,  0.1397,  0.1994, -0.2073,  0.7742,  0.9978,\n",
       "          0.4974, -0.4033, -0.1074,  0.9378,  0.9813, -0.7749,  0.7348, -0.6909,\n",
       "         -0.3219, -0.5288],\n",
       "        [-0.5690,  0.8720,  0.3111, -0.3945, -1.0187,  0.1962,  0.0953, -0.5634,\n",
       "          0.2079,  0.4241, -0.0719, -0.4937, -0.2775,  0.4149, -0.1146,  0.8249,\n",
       "          0.4472, -0.1056],\n",
       "        [ 0.7055,  0.1352, -0.0606,  0.5378,  0.7861, -0.5110, -0.5375, -0.3216,\n",
       "         -0.6660,  0.0518, -0.2574, -0.1374,  0.2959,  0.1700, -0.4750, -1.2977,\n",
       "          0.6809, -0.0407],\n",
       "        [-0.2461, -0.4997, -0.7868,  0.6758,  0.0514,  0.1365,  0.2867, -0.4872,\n",
       "          0.2300, -0.2027,  0.1064, -0.1115, -0.0092, -0.0769,  0.0920,  0.2657,\n",
       "         -0.9941, -0.1773]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(x.long())\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 18])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0780, 0.0355, 0.0507, 0.0141, 0.0780, 0.0552, 0.0402, 0.0385, 0.0477,\n",
       "         0.0338, 0.1076, 0.0560, 0.0219, 0.0529, 0.0463, 0.1056, 0.0708, 0.0673],\n",
       "        [0.0236, 0.0196, 0.0616, 0.0494, 0.0524, 0.0349, 0.0931, 0.1164, 0.0706,\n",
       "         0.0287, 0.0385, 0.1096, 0.1145, 0.0198, 0.0895, 0.0215, 0.0311, 0.0253],\n",
       "        [0.0277, 0.1169, 0.0667, 0.0330, 0.0177, 0.0595, 0.0538, 0.0278, 0.0602,\n",
       "         0.0747, 0.0455, 0.0298, 0.0370, 0.0740, 0.0436, 0.1116, 0.0765, 0.0440],\n",
       "        [0.1038, 0.0587, 0.0483, 0.0878, 0.1126, 0.0308, 0.0300, 0.0372, 0.0263,\n",
       "         0.0540, 0.0396, 0.0447, 0.0689, 0.0608, 0.0319, 0.0140, 0.1013, 0.0492],\n",
       "        [0.0445, 0.0345, 0.0259, 0.1119, 0.0599, 0.0653, 0.0758, 0.0350, 0.0717,\n",
       "         0.0465, 0.0633, 0.0509, 0.0564, 0.0527, 0.0624, 0.0743, 0.0211, 0.0477]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(y_hat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(y_hat, dim=1).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9675, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 14, 14, 14, 14])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  7,  1,  4,  3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(y_hat, dim=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_dl, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for x, y in train_dl:\n",
    "            batch = y.shape[0]\n",
    "            out = model(x.long())\n",
    "            loss = F.cross_entropy(out, y)   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += batch\n",
    "            sum_loss += batch*(loss.item())\n",
    "        train_loss = sum_loss/total\n",
    "        valid_loss, valid_acc = valid_metrics(model, valid_dl)\n",
    "        print(\"train loss  %.3f val loss %.3f and accuracy %.3f\" % (\n",
    "            train_loss, valid_loss, valid_acc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, y in valid_dl:\n",
    "        batch = y.shape[0]\n",
    "        out = model(x.long())\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        pred = torch.max(out, dim=1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.955097976100985, 0.027516075968296694)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_metrics(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss  1.980 val loss 1.349 and accuracy 0.658\n",
      "train loss  1.361 val loss 1.045 and accuracy 0.697\n",
      "train loss  1.170 val loss 0.927 and accuracy 0.734\n",
      "train loss  1.062 val loss 0.847 and accuracy 0.751\n",
      "train loss  0.987 val loss 0.792 and accuracy 0.761\n",
      "train loss  0.944 val loss 0.740 and accuracy 0.778\n",
      "train loss  0.891 val loss 0.700 and accuracy 0.789\n",
      "train loss  0.868 val loss 0.662 and accuracy 0.797\n",
      "train loss  0.841 val loss 0.645 and accuracy 0.798\n",
      "train loss  0.822 val loss 0.628 and accuracy 0.807\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, train_dl, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
